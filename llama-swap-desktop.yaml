# preload all gguf files using:
# cat llama-swap.yaml | grep -- '-hf ' | sed -re 's/.*-hf ([^ ]+).*/\1/'  | sort -u | xargs -i{} llama-server -hf {} -ngl 0 --help

# healthCheckTimeout: number of seconds to wait for a model to be ready to serve requests
# - optional, default: 120
# - minimum value is 15 seconds, anything less will be set to this value
healthCheckTimeout: 500

# logLevel: sets the logging value
# - optional, default: info
# - Valid log levels: debug, info, warn, error
logLevel: info

# metricsMaxInMemory: maximum number of metrics to keep in memory
# - optional, default: 1000
# - controls how many metrics are stored in memory before older ones are discarded
# - useful for limiting memory usage when processing large volumes of metrics
metricsMaxInMemory: 1000

# startPort: sets the starting port number for the automatic ${PORT} macro.
# - optional, default: 5800
# - the ${PORT} macro can be used in model.cmd and model.proxy settings
# - it is automatically incremented for every model that uses it
startPort: 10001

# macros: a dictionary of string substitutions
# - optional, default: empty dictionary
# - macros are reusable snippets
# - used in a model's cmd, cmdStop, proxy and checkEndpoint
# - useful for reducing common configuration settings
macros:
  "latest-llama": >
    nice /app/llama-server
    --offline
    --port ${PORT} --no-warmup
    --cache-type-k q8_0 --cache-type-v q8_0
    --flash-attn on --cont-batching -np 1

  "latest-llama-cpu": >
    nice /app/llama-server-cpu
    --offline -ngl 0
    --port ${PORT} --no-warmup
    --cache-type-k q8_0 --cache-type-v q8_0
    --flash-attn on --cont-batching -np 1

  "latest-ikllama": >
    nice /app/ikllama-server
    --port ${PORT} --no-warmup -ngl 0
    --cache-type-k q8_0 --cache-type-v q8_0
    --flash-attn on --cont-batching -np 1

# models: a dictionary of model configurations
# - required
# - each key is the model's ID, used in API requests
# - model settings have default values that are used if they are not defined here
# - below are examples of the various settings a model can have:
# - available model settings: env, cmd, cmdStop, proxy, aliases, checkEndpoint, ttl, unlisted
models:
  # keys are the model names used in API requests
  "gemma3":
    cmd: |
      ${latest-llama-cpu}
      -hf unsloth/gemma-3-27b-it-qat-GGUF:Q4_0 --ctx-size 0
    name: "Gemma3 27B QAT"

  "gpt-oss-20b":
    cmd: |
      ${latest-llama}
      -hf mradermacher/gpt-oss-20b-Derestricted-i1-GGUF:MXFP4_MOE --jinja --ctx-size 0
    name: "GPT-OSS 20b"
  
  "gpt-oss-120b":
    cmd: |
      ${latest-llama}
      -m /cache/llama.cpp/gpt-oss-120b-Derestricted.i1-MXFP4_MOE.gguf --jinja --ctx-size 0 --cpu-moe
    name: "GPT-OSS 120b derestricted"
  
  "hermes-4.3-36b":
    cmd: |
      ${latest-llama}
      -hf mradermacher/Hermes-4.3-36B-i1-GGUF:IQ3_XS  --ctx-size 0
    name: "Hermes 4.3 36B Q3"

  "precog-24b":
    cmd: |
      ${latest-llama}
      -hf bartowski/TheDrummer_Precog-24B-v1-GGUF:Q4_K_M --ctx-size 0
    name: "TheDrummer Precog 24B v1 Q4"

  "minimax-m2":
    cmd: |
       ${latest-llama-cpu} -hf moxin-org/MiniMax-M2-GGUF:MXFP4_MOE --jinja --ctx-size 0
    name: "Minimax M2 Q4"

  "minimax-m2.1":
    #cmd: ${latest-llama} -hf mradermacher/MiniMax-M2.1-i1-GGUF:Q4_K_M --jinja -ngl 0 --ctx-size 0
    cmd: ${latest-llama-cpu} -hf unsloth/MiniMax-M2.1-GGUF:Q4_K_XL --jinja --ctx-size 0
    #cmd: ${latest-ikllama} -m /cache/llama.cpp/mradermacher_MiniMax-M2.1-i1-GGUF_MiniMax-M2.1.i1-Q4_K_M.gguf --jinja --ctx-size 0
    name: "Minimax M2.1 Q4"


  "intellect-3":
    cmd: |
       ${latest-llama} -hf bartowski/PrimeIntellect_INTELLECT-3-GGUF:Q5_K_S --jinja --ctx-size 0
    name: "Intellect-3 Q5"

  "trinity-mini":
    cmd: |
      ${latest-llama} -hf arcee-ai/Trinity-Mini-GGUF:Q8_0 --jinja --ctx-size 0
    name: "Trinity Mini"

  "qwen3-next":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF:Q6_K
      --jinja --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 --presence-penalty 1.5 -n 256000 --no-context-shift --ctx-size 0
    name: "Qwen3 Next Thinking Q6"
    
  "qwen3-next-instruct":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF:Q6_K
      --jinja --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 --presence-penalty 1.5 -n 256000 --no-context-shift --ctx-size 0
    name: "Qwen3 Next Instruct Q6"
    
  "qwen3-embed":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0
      --pooling last -ub 8192 --verbose-prompt --embeddings
      --ctx-size 0
    name: "Qwen3 Embedding"
  
  "nomic-embed":
    cmd: |
      ${latest-llama}
      -hf nomic-ai/nomic-embed-text-v1.5-GGUF:F16
      --pooling last -ub 8192 --verbose-prompt
      --ctx-size 0 --embeddings
    name: "Qwen3 Embedding"
  
  "qwen2.5-coder-7b":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen2.5-Coder-7B-Instruct-GGUF:Q8_0 --ctx-size 6192
    name: "Qwen2.5-Coder-7B Q8"
    filters:
      strip_params: "suffix"

  "qwen2.5-coder-3b":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen2.5-Coder-3B-Instruct-GGUF:Q8_0 --ctx-size 6192
    name: "Qwen2.5-Coder-3B Q8"
    filters:
      strip_params: "suffix"

  "qwen2.5-coder-1.5b":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF:Q8_0 --ctx-size 6192
    name: "Qwen2.5-Coder-1.5B Q8"
    filters:
      strip_params: "suffix"

  "qwen3-rerank":
    cmd: |
      ${latest-llama}
      -hf DevQuasar/Qwen.Qwen3-Reranker-0.6B-GGUF:Q8_0 --ctx-size 0
    name: "Qwen3 Reranker Q8"

  "zerank-1-small":
    cmd: |
      ${latest-llama}
      -hf mradermacher/zerank-1-small-GGUF:Q8_0 --ctx-size 0
    name: "Zerank 1 Small Q8"

  "youtu-llm":
    cmd: |
      ${latest-llama}
      -hf mradermacher/Youtu-LLM-2B-GGUF:Q8_0 --ctx-size 0
    name: "Youtu-LLM-2B Q8"

groups:
  # group1 works the same as the default behaviour of llama-swap where only one model is allowed
  # to run a time across the whole llama-swap instance
  "big":
    # swap: controls the model swapping behaviour in within the group
    # - optional, default: true
    # - true : only one model is allowed to run at a time
    # - false: all models can run together, no swapping
    swap: true

    # exclusive: controls how the group affects other groups
    # - optional, default: true
    # - true: causes all other groups to unload when this group runs a model
    # - false: does not affect other groups
    exclusive: false

    # members references the models defined above
    # required
    members:
      - "gpt-oss-120b"
      - "hermes-4.3-36b"
      - "precog-24b"
      - "intellect-3"
      - "minimax-m2"
      - "minimax-m2.1"
      - "gpt-oss-20b"
      - "gemma3"
      - "trinity-mini"
      - "qwen3-next"
    
  "autocomplete":
    swap: true
    exclusive: false
    members:
      - "qwen2.5-coder-3b"
      - "qwen2.5-coder-7b"
      - "qwen2.5-coder-1.5b"
      
  "embed":
    swap: true
    exclusive: false
    members:
      - "qwen3-embed"
      - "nomic-embed"
      
  "rerank":
    swap: true
    exclusive: false
    members:
      - "zerank-1-small"
      - "qwen3-rerank"

  # Example:
  # - in group2 all models can run at the same time
  # - when a different group is loaded it causes all running models in this group to unload
  # "group2":
  #   swap: false

    # exclusive: false does not unload other groups when a model in group2 is requested
    # - the models in group2 will be loaded but will not unload any other groups
    # exclusive: false
    # members:
    #   - "docker-llama"
    #   - "modelA"
    #   - "modelB"

  # Example:
  # - a persistent group, prevents other groups from unloading it
  # "forever":
    # persistent: prevents over groups from unloading the models in this group
    # - optional, default: false
    # - does not affect individual model behaviour
    # persistent: true

    # set swap/exclusive to false to prevent swapping inside the group
    # and the unloading of other groups
    # swap: false
    # exclusive: false
    # members:
      # - "forever-modelA"
      # - "forever-modelB"
      # - "forever-modelc"

# hooks: a dictionary of event triggers and actions
# - optional, default: empty dictionary
# - the only supported hook is on_startup
hooks:
  # on_startup: a dictionary of actions to perform on startup
  # - optional, default: empty dictionary
  # - the only supported action is preload
  on_startup:
        # preload: a list of model ids to load on startup
        # - optional, default: empty list
        # - model names must match keys in the models sections
        # - when preloading multiple models at once, define a group
        #   otherwise models will be loaded and swapped out
    #preload:
    #  - "glm-4.5-air"
